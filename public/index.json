[{"authors":["Meghan Hall"],"categories":null,"content":"Meghan Hall is a higher education data professional by day and an amateur hockey analyst by night. She contributes to Hockey-Graphs and has written for Nightingale, the publication of the Data Visualization Society. She is also an enthusiastic user of both R and Tableau and loves helping other people learn these tools to make their work easier.\nShe presented at the Seattle Hockey Analytics Conference in March 2019 on league-wide trends in goalie pulling and has presented on features of the penalty kill three times, at the Rochester Institute of Technology Sports Analytics Conference in August 2019, the Ottawa Hockey Analytics Conference in November 2019, and the Columbus Blue Jackets Hockey Analytics Conference (with Alison Lukan) in February 2020. She also presented virtually at Hockey (Analytics) Night in Canada in April 2020 on how and why to learn a programming language.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Meghan Hall is a higher education data professional by day and an amateur hockey analyst by night. She contributes to Hockey-Graphs and has written for Nightingale, the publication of the Data Visualization Society. She is also an enthusiastic user of both R and Tableau and loves helping other people learn these tools to make their work easier.\nShe presented at the Seattle Hockey Analytics Conference in March 2019 on league-wide trends in goalie pulling and has presented on features of the penalty kill three times, at the Rochester Institute of Technology Sports Analytics Conference in August 2019, the Ottawa Hockey Analytics Conference in November 2019, and the Columbus Blue Jackets Hockey Analytics Conference (with Alison Lukan) in February 2020.","tags":null,"title":"Meghan Hall","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":"    When I created a tutorial with swirl earlier this year, some people asked me why I hadn't instead used learnr, another tool for interactive learning. I wasn't familiar with learnr, but since it's based in R Markdown and one of my goals for this year is to improve my skills in that area, I decided to give it a try!\nSo I've created two tutorials in learnr, one that is a true introduction for beginners (focused on the tidyverse) and one that goes a little bit deeper into data manipuation with pivoting and joining data. These tutorials (screenshot below) live in a package I created called betweenthepipes. More detail on the package and the tutorial content is available on my Github.\n   To access these tutorials, simply download the package. If you're working in RStudio 1.3 or later, there should be a Tutorial pane in the upper right corner of the program where you should see the two tutorials listed. If not, or if you'd prefer to access the tutorials directly, follow the second block of code below.\n# Install my package via github install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;meghall06/betweenthepipes\u0026#34;) # Only necessary if you don\u0026#39;t have RStudio \u0026gt;= 1.3 # Or if you want to access them directly learnr::run_tutorial(\u0026#34;intro\u0026#34;, package = \u0026#34;betweenthepipes\u0026#34;) learnr::run_tutorial(\u0026#34;data_manip\u0026#34;, package = \u0026#34;betweenthepipes\u0026#34;)","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593388800,"objectID":"695a1da3bfb705d745b5b5e352e36adb","permalink":"/post/learnr/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/post/learnr/","section":"post","summary":"When I created a tutorial with swirl earlier this year, some people asked me why I hadn't instead used learnr, another tool for interactive learning. I wasn't familiar with learnr, but since it's based in R Markdown and one of my goals for this year is to improve my skills in that area, I decided to give it a try!\nSo I've created two tutorials in learnr, one that is a true introduction for beginners (focused on the tidyverse) and one that goes a little bit deeper into data manipuation with pivoting and joining data.","tags":null,"title":"Learning R with Hockey Data and learnr","type":"post"},{"authors":[],"categories":[],"content":" When people ask me how to get into sports analytics, I always suggest starting with a question that they’re interested in exploring and using that question as a framework for learning the domain knowledge and the technical skills they need. I feel comfortable giving this advice because it’s exactly how I got into hockey analytics: I was curious about goalie pulling, and I couldn’t find enough data to satisfy my curiosity. There are plenty of articles on when teams should pull their goalies, but aside from a 2015 article on FiveThirtyEight by Michael Lopez and Noah Davis, I couldn’t find much data on when NHL teams were actually pulling their goalies and if game trends were catching up to the mathematical recommendations. I presented some data on the topic at the Seattle Hockey Analytics Conference in March 2019, but the following analysis is broader and includes more seasons of data.\nRead the entire article on Hockey-Graphs here.\nThe Tableau dashboard that accompanies this article should show below and is also available here.\n  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"3a28117a2afe05753f06fe0b32d22c43","permalink":"/post/goalie-pulling/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/goalie-pulling/","section":"post","summary":"When people ask me how to get into sports analytics, I always suggest starting with a question that they’re interested in exploring and using that question as a framework for learning the domain knowledge and the technical skills they need. I feel comfortable giving this advice because it’s exactly how I got into hockey analytics: I was curious about goalie pulling, and I couldn’t find enough data to satisfy my curiosity.","tags":[],"title":"The State of Goalie Pulling in the NHL","type":"post"},{"authors":[],"categories":null,"content":"","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587168000,"objectID":"766f40e7f2d6b8c5dbae3018f118333c","permalink":"/talk/hanic/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/talk/hanic/","section":"talk","summary":"Moving beyond Excel to make your analysis more reproducible, efficient, and shareable.","tags":[],"title":"Moving Beyond Excel for Your Hockey Analysis","type":"talk"},{"authors":[],"categories":[],"content":"  Screencast video Introduction (0:35) Dimensions and measures (3:45) Building a bar chart (5:25) Side-by-side bar chart (15:30) Stacked bar chart (23:35) Line graph (28:30) Scatterplot (33:45) Chart with dual axis (37:38) Pie chart (45:35)   Screencast video A bunch of people have been asking me for some kind of Tableau tutorial, so I’m experimenting with a screencast video on YouTube! The video should be embedded below and is also available here. The sample data set I use will probably be familiar to you if you’ve ever done any of my R tutorials, it’s a CSV that’s available to download here (to download from Github, just hold down Alt and click the Raw button). A (lightly-edited) transcript with time stamps is available below.\n  Introduction (0:35) Just as a disclaimer before we get started, I had mentioned that I had done a lot of R tutorials, and there are a lot of people who want to discuss which is better and which is worse, Tableau vs. R vs. many other programs for visualizing data, and I personally don’t really have a lot of interest in that argument. Both Tableau and R have strengths and weaknesses, advantages and disadvantages, and I really don’t think there’s any harm in having more tools in your toolbox. Tableau in particular is really useful free tool for visualizing data and is particularly beginner-friendly.\nThis is Tableau Public. Tableau does have a lot of various enterprise options, but Tableau Public is the version that is free. The main limitation with Tableau Public is that you cannot save anything locally to your computer. Along with downloading Tableau Public you get a Tableau Public account, and everything gets saved to that account. It is true that you can hide visualizations, I have certain older visualizations that I don’t really want anyone to see anymore. I have them hidden but they still exist on the Internet, so this is not a good solution if you are working with confidential data or data that you don’t want anyone to see.\nWe’re going to start off by connecting to data, which you can do with this big Connect to Data button. The data we’re going to use for this is the data that I use for most of my tutorials, so I’m sorry if you’re bored with this data. It is a CSV file that’s available on my Github, it’s hockey data, four games from the past NHL season, four Philadelphia Flyers games from November 2019. It’s play-by-play data I have scraped, and that’s what we’ll be using here.\nOnce you load in data here, we’re just going to be working with one csv file, but you can bring in more files and you can do joins and unions and all of those sorts of things. You can also modify your data fields here in this pane, you can do simple pivots to make your data wider or longer. You can also modify your data fields within the worksheets, which we’re going to do now. Anytime you want to get back to your data source, just select the Data Source button in the lower left corner.\n Dimensions and measures (3:45) A big part of learning Tableau is just understanding how it works, and once you understand that, you can more intuitively understand how to build visualizations. The hardest part of learning Tableau is figuring out what they call everything and how to use their language to build the visualizations that you need. But in general, here it will show all your data fields as dimensions, which are string or character variables, and measures, which are numbers. Tableau makes its best guess when it reads in data, but you can make changes. In this example, it read our season variable as a number, but I don’t really want it as a number, I would prefer that it be a string variable. All you have to do is right-click on the data field and select Change Data Type, and you can change it to a string. It will still leave it as a measure, but all you have to do is drag it up to the dimension section. You can do the same the other way, as well. This data set contains x and y coordinates for shot locations, and while it automatically brought them in as strings, maybe we want them as numbers. Again, just select both, right click, select Change Data Type and select whole number. Then move them to measures.\n Building a bar chart (5:25) The first thing to learn in any data visualization program is how to make a super-simple bar chart. We’re going to look at the number of shots each player took. If you just double-click any variable, like we do here with Event Player 1, it will automatically bring it over into the view onto Rows. We’re also going to filter this view because we’re only interested in shots on goal for this particular bar graph. So we’re going to drag the Event Type field to filter and then select GOAL and SHOT. You can get fairly complicated with your filters (that will be an overarching theme in this entire tutorial, everything I show you does have more complex options), but we’re just choosing two.\nOne of the most useful fields in Tableau is this Number of Records field, which will show up in every data set under measures. It just counts the number of rows and we’ll bring that in by double-clicking it. Now we have a list of players and their shots. We probably want to sort this, so we’re going to right-click on this Event Player 1 field (in Tableau these are called pills) and select Sort. There are lots of ways to sort, but we’re going to sort by field, by descending, and by the default, which is the sum of the number of records.\nOne of the nice things about Tableau is that it’s got this icon in the upper right called Show Me that has a bunch of different graph types you can select. Now when I build visualizations in Tableau I don’t really use these because I know how to build the charts and graphs that I want, but it’s really useful for beginners because it teaches you how Tableau builds visualizations. If I click on bar graph, watch what happens on the screen: all it did was move Number of Records from text to columns.\nOne of my biggest tips for people who are learning Tableau is to make as many changes as you can to these default settings because everyone’s default Tableau graph looks the same. Every one looks the same, with this font and this tooltip (which is what shows up when you hover over a data point) and the blue bars. There’s a lot of customization you can do in Tableau, and as we go through these several chart types that we’re going to build, we’re going to show some of those options. The first thing I’m interested in is I want to have a label on these bars. All I have to do is drag my Number of Records field to the label field. If I want to make other changes, you can go into the Format menu, then select Workbook. When you want to make changes, you can do it either at the full workbook level (which I would recommend for changes you’ll want everywhere, like changing the font) or the individual worksheet level. One reminder: if you’re sharing these visualizations on Tableau Public, which most people are, you can’t necessarily use tons of fancy fonts because the people viewing your visualizations might not have those fonts. There’s a list of fonts that are global on Tableau Public, which you can find via Google, but right now just to make a change, we’re using super-plain Arial. This will change the font throughout the entire workbook. If there are individual changes you want to make, you can change the font for those things separately and it will override the global option.\nJust to show how that would work, if we wanted to modify these row headers, all we have to do is right-click and select Format. Now we can change the font, we can make it bold and larger, etc. I personally don’t like these field labels, so I always right-click and hide those.\nThis is kind of a big bar chart, so if I only want to show players with at least five shots, all you have to do is right-click on the Number of Records field and select Filter. We’re going to select At Least and filter to only show players that have at least five shots, and that will make the bar chart much smaller. Again, the standard bar chart color is always blue, so if you want to change what this bar color is, you have a few options. You can select the color field here in the Marks section, or you can bring in another data field and drop it onto this color option. Say that we want to take this Event Team field, we can drag it over to the color option. Blue and orange is the standard Tableau color palette, so I would highly recommend changing it. Just select the colors, you can add custom colors by hex codes, you can upload your own color palettes, or you can use a Tableau color palette. We’ll make Philadelphia orange and make Vancouver blue. This is a pretty ugly graph now, but it shows you some of the things you can do when making a bar chart.\nThe next thing I would want to change is this axis so that it actually reads something meaningful. You can do this by right-clicking on the axis and selecting Edit. You can change what you want the range of the axis to be, since in some cases you might not want to show zero, I’m just going to change the axis title to Total Shots on Goal. I personally also have a lot of opinions, like most of you probably do, on how I want my graphs to look, and Tableau allows you to customize just about everything with how the lines and axes look. You can change these on each individual sheet, but I prefer to do it by going to Format -\u0026gt; Workbook because I know I like these changes across the workbook. I generally don’t like grid lines so I can take those off, I can get rid of axis ticks which I also don’t like, and I can change up what the axis lines look like. Again, there are a lot of different formatting options, but I recommend using the Format -\u0026gt; Workbook method for the global options you want because it will make it much easier.\nLastly, one of the nice things about Tableau that I mentioned before is that you have these tooltips if you hover over a data point. These are very customizable and I recommend that you customize them because there are a lot neat things you can do with them, and it makes your Tableau workbook look a little more custom, a little less straight-out-of-the-box.\nIf you don’t like the tooltips in general, you can find them by clicking on the Tooltips button on this marks card, you can just unclick “Show tooltips” and it will just remove them. But again, you can really play with these. All of the fields that are in your view are available by clicking the Insert button, and you can also add more fields to your view if you wanted to have access to them in your tooltip. We could rewrite these, and I think it’s nice to have more detail in your tooltips.\n Side-by-side bar chart (15:30) We’ll create a new sheet down here, and we’ll take our Game ID field (a unique identifier for each game) into Rows. We’re again going to use our Event Type filter and filter down to to SHOT and GOAL. A neat thing with Tableau filters (though we’re not going to experiment with this here) is that you can apply these filters to multiple worksheets, which is really useful so you don’t have to keep creating filters. We’re going to add the number of records onto Columns, which will make a bar chart. Up here in the menu bar, there’s an option to swap rows and columns to make a column bar chart. Right now this is showing the total number of shots on goal, for both teams, for each game. If I wanted to see it by team and create a side-by-side bar chart, all I have to do is drag this Event Team data field over to Columns, then it automatically creates a side-by-side bar chart for me. Here at the bottom are all of the teams in these games, and the only thing I don’t like about this is that I’d like it better if the Philadelphia column was to the right in every section. It’s not because it’s showing the teams in alphabetical order, but we can change this by going to the Event Team data field, click the arrow and then click sort, we can sort manually and I can drag Philadelphia to the bottom so that it’s the right-hand column in each of these sections, which I find easier to read.\nSay we want to color these bars by which team it is. We can do that by bringing this Event Team dimension, just dragging it over to color on the Marks card. I don’t particularly like this because it assigned a different color to each team, which I don’t think for this visualization is necessary. If I wanted all the other teams besides Philadelphia to be gray, I could just go through and change every other team color manually, but that’s a little too much work. Instead I’m going to create a calculated field that we’re going to keep using through this tutorial. Under the Analysis menu, go to Create Calculated Field. You can create a lot of different calculations in Tableau, but this is going to be a super simple if statement: IF Event Team = “PHI” THEN “Y” ELSE “N” END. Drag this new PHI calculation onto color, replacing the Event Team pill that was already there. This is better because Philadelphia is one color and all the other teams are another color. I’m going to change the specific colors, also. The good news is when you change colors in Tableau, those colors will maintain every time you use the field. Here we’re setting the Philadelphia color to be orange and the “other team” color to be light gray. If you use this field in another visualization, which we will do, it will maintain these colors.\nJust as a reminder, you can format any of these options that are here in the view. If we wanted to change this Game ID field, we could right-click and select Format to make this bold, if we wanted to make Event Team italic, we can do that. Another neat feature in Tableau when you’re dealing with dimensions is the concept of aliases. If, for example, I don’t want these Game ID values to show up like this, all I have to do is right-click on it and I can edit the alias. I could call this “Carolina Game” or “Game 27” or whatever. These aliases can be really useful if the way your data comes in doesn’t necessarily match the way you want to display it. If you want to get rid of abbreviations and things like that, the aliases can help. You can also find them if you navigate to that data field in the dimensions pane, right-click and select Aliases. You can set them all at once.\nOne last thing that I wanted to show on this particular graph is the way that filters work. This graph is showing the number of shots on goal for every team in every game, but say we wanted to filter that further, we can drag in this Game Strength State field onto Filter. Say I only wanted to see shots that happened at 5v5, or I can go over to the data field in the Filter section, hit the arrow, and select Show Filter. That will add it to the view. Again, there are a lot of formatting options for the filter which you can find by selecting the little arrow in the title section. For example, I can select Single Value and List, which allows you to easily change the filter selections in your view.\n Stacked bar chart (23:35) We’ll open another new sheet and will work with Game ID again. We’ll bring Game ID to columns, and we’ll use our favorite Number of Records field and we’ll bring that to Rows. This has automatically made a column bar chart with all of the rows for each Game ID. We are not interested in all of those Event Types, so we’ll bring Event Type to Filter and filter down to just shot attempts: BLOCK, GOAL, MISS, SHOT. Let’s say we want to create a stacked bar chart where I can see the percentage of each event type for each game. So how we would build this is that we need to take the Event Type field again and bring that over to Color, which will add the color feature to our bar chart with a different color for each event type. We’ll change the color palette, I mentioned there are a bunch of built-in color palettes within Tableau, or you can also create your own. This is closer to what we want, but this is still showing total numbers while we want to see a percentage.\nIn order to do that, we need to go up to our Number of Records pill on our Rows shelf, click the little arrow, and we’ll go down to the table calculation section. I know I sound like a broken record here, but here in this tutorial we’re just skimming the surface of what Tableau can do. You can get pretty complicated with your table calculations, but we’re just going to select Quick Table Calculation and then Percent of Total. Now these percentages don’t look exactly right, and that just means they aren’t computing along the right data field. So all we have to do is click the arrow on the Number of Records pill and you’ll see a new option in this menu called Compute Using. This tells Tableau how you want this percentage to be calculated. We want it to be calculated across Event Type, so we’ll select that, and then that will make the stacked bar that we want.\nIf we want to add a label to these bars, all we have to do is again, select the Number of Records field from the measures section and drag that over to Label. That will label all the individual sections, but it just labels it with the raw number. You might want to show that (I often show the raw number as well as the calculation in my labels), but if we just want the percentage label, add the same Quick Table Calculation to this label. You can select the Format option on the Label pill if you wanted to change the number format. The labels, similar to the tooltips, are very flexible and you can add a lot of information. If we also wanted to add Event Type to the label, we could absolutely do that. Drag in another instance of the Event Type field and drag it onto Label. It will show up automatically, and if you want to edit it further, just click the Label card to edit and format the text.\n Line graph (28:30) We have a Game Date field here. Working with dates in Tableau, just like working with dates in any other program, has a bit of a learning curve to it. A lot of that depends on how your dates are formatted in the data source as you bring them into Tableau, but Tableau has a lot of functionality around dates. Here, we’re going to bring our Game Date field onto Columns. You can see kind of immediately that this looks weird, this just shows the year of the Game Date. But we can change that by selecting the down arrow on the pill and there are lots of options for how to display the date. We want to display the full date, so we’ll select Day. These are the four dates that these games were played. This also looks a little weird, but we’re going to fix that in a minute. Also, one of my biggest Tableau lessons is don’t be afraid when your visualization looks weird because a lot of them look weird and bad at the beginning and just take some finagling to make them look good. This looks weird, don’t worry about it.\nWe’re going to move our Number of Records field to Rows, and that’s automatically going to make a basic line graph. This does this because it recognizes the axis as a date. Over on the marks card, you can see that the line option is the automatic option. You can change this here, we could make it into a bar chart, we could make it into circles, but we’re good with the Line option. These games dates aren’t showing up exactly as how I want them because it’s a little misleading to add these extra dates in here. These four games were only on the odd number dates, and it’s incorrect data-wise to be showing these other dates. I understand why Tableau is doing this, it thinks it’s being helpful, but it’s really not. That’s another common thing with Tableau, you often have to figure out ways to override the ways that it tries to help you that you don’t agree with. We can fix this particular problem by going back to the Game Date pill, selecting the down arrow, and then change it to Discrete from Continuous. Then you only have the selected dates, which is exactly what we want.\nThis is just showing all the rows per Game Date, but we can add Event Type to Filter again and just select BLOCK, GOAL, MISS, SHOT. This shows a line graph for both teams during the game, but if we wanted to, we could add something to the Color mark to make multiple lines. If we go back to the data field we created earlier, which is just a yes/no field that determines whether the team is Philadelphia or their opponent, and bring that to Color, you can see here that it creates multiple lines.\nThe last thing I want to show on this graph (again, we’re just trying to touch on a lot of the basic Tableau functionalities) is that I want to introduce the concept of a reference line. If you right click on any numeric axis, there should be an option to Add Reference Line. Again, you can get very complex with your reference lines, but this one is going to be really basic, so basic that it’s the default reference line. It’s taking the value of that axis, which here is the sum of the Number of Records, and we want to show the average. We’re going to change the scope up here to the Entire Table, and you can see that it creates an average line. You can get very fancy with how you calculate these reference lines, but we’ll keep it really simple for now. You can change the label, if I want it to say Average Number of Shots, you can set what the tooltip looks like, you can format what the line looks like, if we want an orange line like this. This is a very useful feature in Tableau that makes it easy to look at data aggregated at different levels within the same view. There are much more complex ways to do that, we’ll very briefly touch on it, but this is a useful way if the measure you’re interested in is fairly simple.\n Scatterplot (33:45) We have been looking, in some of our previous charts, at our Event Type field filtered down into different groups. But for a scatterplot, we’re going to make a couple of calculated fields so that we can compare them. If we go to the Analysis menu, then Create Calculated Field, all we’re going to do is create two calculated fields, one is going to isolate the GOAL values and the other will isolate GOAL and SHOT. You can do that individually by filtering the view, as we’ve been doing, but since we want to compare them both at the same time in the same view, it’s just easier to create some simple calculated fields. Our Goals calculated field: IF Event Type = “GOAL” THEN 1 ELSE 0 END. Our Shots calculated field: IF Event Type = “GOAL” OR Event Type = “SHOT” THEN 1 ELSE 0 END.\nMove the Shots data field to Columns, the Goals data field to Rows. This does not look like a standard scatterplot, and that’s because Tableau as a default likes to aggregate data. This one little dot that shows up on this pathetic scatterplot shows all the goals and all the shots. That’s not what we’re interested in for this particular exercise, we want to see a scatterplot of shots versus goals for every team in each game. So all we have to do in order to do that is add those fields over here onto the Detail option on the Marks card. If we drag Game ID onto Detail, now we have four dots, one for each game, which is progress. If we also add Event Team onto detail, there we go, now we have eight dots. This particular dot at 0,0 is showing up because there are some rows with an Event Team of NA. To get rid of those, all we have to do is select our Event Team pill, click Filter, and uncheck NA.\nNow we have a scatterplot that isn’t formatted in any customized way but looks how we want it to look. If we wanted to make these dots into filled-in circles, as opposed to empty circles, go onto the Marks card and change to Circle. We can also make them bigger by adjusting the size, and if we want to do color, we can use our PHI calculated field and drag that onto color. You can see that this maintains the color palette we created for this field previously. All of the Philadelphia circles are orange, and the other teams are gray. You can also add labels, if we wanted to add Event Team, we can just drag that field onto Label, and then you could customize those.\n Chart with dual axis (37:38) This next one we’re going to build is a really helpful skill to learn in Tableau. We’re going to learn how to make what’s basically a bar chart but with a dual axis. We’re going to do this in a really simple way, and this is a good gateway into how to make really neat-looking custom graphs in Tableau. The dual axis is a really powerful feature to know. A big learning curve is Tableau is learning how to display data at different levels of aggregation in the same view. We showed it before when we made the reference line in our line graph, that’s one really easy way to do it, this dual axis method is a way to explore it, and you can also do this with calculations (and that won’t be the focus of this video because it’s getting a little too complex).\nHere, we’re going to make a dual axis chart that shows how many shots a player got in all of these games, compared to what their average per game was. So we’re looking at different levels of aggregation in the same view. In this example, I’m only interested in the Philadelphia players because they played the most games, so I’m going to drag Event Team over to Filter and select Philadelphia. I’m going to filter my Event Type to shot attempts (BLOCK, GOAL, MISS, SHOT), and then I’m going to bring my Event Player 1 field onto Columns. This should be old hat now, we’re going to add the Number of Records to text and we’re going to sort it (again, basically the same as how we built the bar chart in the beginning).\nOn our way to building this dual axis chart, I’m going to show how you can make basic cross tabs in Tableau, which are very useful for looking at your data. So again, it’s going to look a little ugly for a minute. We’re going to take this Measure Names field, which is always the last field in the dimensions section and just has the names of every measure, and bring that up into Columns. We’re going to take the partner of the Measure Names field, the Measure Values field, and we’re going to bring that over onto Text, to replace the Number of Records field. (But you’ll notice that the sort maintains because we did it at the Event Player 1 level.) I’m not interested in all of these fields, so I can just select Filter on the Measure Names pill and uncheck all of them besides Number of Records. You’re going to question why I did this, but it makes it easier to show you the next part. So I mentioned that for this dual axis chart, we want to see the total number of shot attempts for each player, which we have listed here already, and we also want to see the average per game. So in order to do that, we need to create an average field. But first, we want to be able to double-check how many games they played. It’s nice, when you’re creating calculations in Tableau, to create them iteratively like this to make sure it’s aggregating as you expect and that the math is right.\nWe’re going to take our Game ID field, which is our unique identifier for each game, and we’re going to drag that into this Measure Values box. Tableau does not like that, as you can see, because we have Game ID set up as a dimension but are asking it to act like a measure. We can get around this, all you have to do is select the Game ID pill and go down to Measure, then we’re going to ask it to, instead of acting as a string, act as a measure with a distinct count. We can see that most of these players played in all four games, a couple only played in three, etc. We’re going to go to the Analysis menu, Create Calculated Field, and we’re going to call this Player Average: SUM(Number of Records) / COUNTD(Game Id). That will create a new data field that we can drag into our Measure Values box, and you can spot check that it is calculating correctly.\nNow we’re going to bring Number of Records to Columns, we’re also going to take Player Average and also bring that to Columns. Now you basically have two bar charts, one for Number of Records and one for Player Average. We’re going to right-click on the Player Average axis and select Dual Axis, which will bring them together. The most important step is to again right-click on the Player Average axis and select Synchronize Axis, so the axes are lined up properly. In this case I don’t really want to show this Player Average axis, so I’ll just right-click and uncheck this Show Header option, which will get rid of it. As you can see now here on the Marks card, we have two options, we have our Number of Records axis, and we have our Player Average marks. We can change both of those. I’m going to take the Number of Records section and change it to a bar, I want it to be a bar chart. I can change the color up here, I want it to be a light gray bar. And then in my Player Average section, I like it as a dot, we can stick with this. You could change this to another shape, you could add a label. When you have a dual axis, you just have to remember that you have two options on your Marks card, so you can make changes to both of them or just to one of them. But that’s another easy way to show data aggregating at different levels: the total and the average.\n Pie chart (45:35) The last chart we’re going to make, we’re going to go through how to make a pie chart. I know pie charts are fairly controversial, we’re going to make a very basic pie chart, mostly just to show you how to do it in Tableau. We’ll filter down to one game, we’ll pick this second one, and we’re going to make a pie chart that shows the shot attempts by team, percentage-wise. We’ll bring Event Team down to color, and again, we want to get rid of the NA value. We can just right-click on the NA value in the color legend and select Exclude, which is, as you can see, just another way to create a filter. Under the Marks card, we’ll change the option to Pie. This makes a half-and-half pie chart. We can use the size option to make it bigger if we want to, but we have to add another field to tell Tableau how we’d like it divided.\nAs you can see when you select Pie on the Marks card, there’s a new option for Angle. All we’re going to do is take our Shots calculated field we created before and drag that right onto Angle. If we wanted to add a label, all we’d have to do is bring our Shots field onto Label, and we now know how to make quick table calculations in Tableau if we wanted a percent of total. We can also format it to show a rounded percentage. A good thing to know about Tableau is that the labels are movable, so you can just move that label straight onto the pie chart if you want. We can change the formatting of the label, we can make it bold, say if we wanted to make it white, we can do that. A compromise that people have reached in the pie chart world is just showing one piece of the pie chart. Say we were only interested in showing the Philadelphia part, this 59 percent section. The easiest way to do that in Tableau, really, is just change the color of the other part of the pie chart and change it to white. Then it looks like you have just one piece of the pie chart.\n ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585526400,"objectID":"fd841889c3249b603bc7c00356d1105a","permalink":"/post/introduction-to-tableau/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/introduction-to-tableau/","section":"post","summary":"A beginner-friendly introduction to Tableau with a screencast on YouTube. Learn the basics of Tableau (dimensions vs. measures, formatting, introductory calculations, tooltips) as well as details on how to create several different chart types, including stacked bar charts, charts with a dual axis, and more.","tags":[],"title":"Introduction to Tableau","type":"post"},{"authors":[],"categories":[],"content":" In March 2020, the Greater Boston useR Group hosted a Virtual Tidy Tuesday Meetup, and I gave a 10-minute lightning talk on getting started with building custom themes in ggplot2. The slides should be embedded below!\n ","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585094400,"objectID":"134e017b8ed72b6026518cb7e39dfe21","permalink":"/post/creating-custom-themes-in-ggplot2/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/creating-custom-themes-in-ggplot2/","section":"post","summary":"Slides from a useR group lightning talk on getting started with creating custom themes in ggplot2.","tags":[],"title":"Creating Custom Themes in ggplot2","type":"post"},{"authors":null,"categories":null,"content":"The swirl package is an incredibly neat learning tool that teaches you how to use R via interactive learning in the RStudio console. And an associated package called swirlify allows anyone to create lessons that can then be used by anyone using swirl.\nI've created a course called Hockey Data With Swirl that aims to teach you basic tidyverse functions using hockey data. The data set used in the swirl lesson is the same one used in my introduction to R at Hockey-Graphs, and the content is similar, but not quite identical. You can go through either tutorial, or both, in any order. The swirl package is just another learning tool that guides you through, question by question. Right now there is only one lesson in the course, but my goal is to add more in the future (such as a lesson for ggplot2!).\nIn order to go through the lesson, you need to have downloaded R and RStudio (the instructions for which are available in the Hockey-Graphs tutorial I linked above). In RStudio, open a script and run the following code:\ninstall.packages(\u0026#34;swirl\u0026#34;) library(swirl) install_course_github(\u0026#34;meghall06\u0026#34;, \u0026#34;Hockey_Data_With_Swirl\u0026#34;) swirl() The console prompts will take over from there. swirl works through the console, which allows for interactivity but doesn't make it easy to save your code and go back to it later. I've attempted to account for that by making available the code file that contains all of the questions and answers from the lesson.\nI hope you find the lesson useful, feel free to give feedback and let me know what other topics you'd be interested in!\n","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"fbb6d2ee33f806a294220ef63079d56e","permalink":"/post/hockey-data-with-swirl/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/post/hockey-data-with-swirl/","section":"post","summary":"The swirl package is an incredibly neat learning tool that teaches you how to use R via interactive learning in the RStudio console. And an associated package called swirlify allows anyone to create lessons that can then be used by anyone using swirl.\nI've created a course called Hockey Data With Swirl that aims to teach you basic tidyverse functions using hockey data. The data set used in the swirl lesson is the same one used in my introduction to R at Hockey-Graphs, and the content is similar, but not quite identical.","tags":null,"title":"Learning R with Hockey Data in Swirl","type":"post"},{"authors":null,"categories":null,"content":"Before we start, an important disclaimer: this is not a tutorial on how to thoughtfully build and thoroughly evaluate models. This is a gentle introduction to the tidymodels package (which, like the tidyverse, is actually a collection of packages), and in order to examine various functions and capabilities of those packages, we'll build two very simple models, using easily available NHL data, and go over a few ways to evaluate them.\nThe tidymodels package, which is fairly new, was designed to make it easier to create your model framework in a tidy way and consists of, among others, recipes (prepping models), parsnip (executing models), and yardstick (evaluating models). Here, we'll build two simple models to predict whether an NHL player is a forward or a defenseman.\nFind and prepare the data First, let's get our data prepped—we're using 2018-19 data so we can have a full season. We'll get the position data by downloading from Natural Stat Trick, and we'll create our statistics from the raw play-by-play data, available via the Evolving Wild scraper. (Could you download all these summary statistics from NST instead? Definitely. But this is about learning, and it's great R practice [pRactice?] to generate them yourself from the play-by-play data.)\n One of the tidymodels packages called dials has a margin() function that will mask the margin() function in ggplot2. If you use the margin() function in your ggplot2 custom theme like I do, just load tidymodels before tidyverse and you should be fine.   library(tidymodels) library(tidyverse) # Read in files (pbp from Evolving Hockey, bios from Natural Stat Trick) pbp \u0026lt;- read_csv(\u0026#34;pbp_20182019_all.csv\u0026#34;) bios \u0026lt;- read_csv(\u0026#34;bios_1819.csv\u0026#34;) # Find player TOI and games played # To do so, you must pivot the data so there is one row per player # (instead of one row per event) # We don\u0026#39;t care about the ice time for the goalies (sorry, goalies) # so they will be filtered out # We also do some name changes to make things easier later player_TOI \u0026lt;- pbp %\u0026gt;% filter(event_length \u0026gt; 0) %\u0026gt;% select(game_id, event_length, home_on_1:away_goalie) %\u0026gt;% pivot_longer(home_on_1:away_on_6, names_to = \u0026#34;variable\u0026#34;, values_to = \u0026#34;player\u0026#34;) %\u0026gt;% filter(!(is.na(player)) \u0026amp; player != home_goalie \u0026amp; player != away_goalie) %\u0026gt;% mutate(player = case_when( player == \u0026#34;COLIN.WHITE2\u0026#34; ~ \u0026#34;COLIN.WHITE\u0026#34;, player == \u0026#34;ERIK.GUSTAFSSON2\u0026#34; ~ \u0026#34;ERIK.GUSTAFSSON\u0026#34;, player == \u0026#34;PATRICK.MAROON\u0026#34; ~ \u0026#34;PAT.MAROON\u0026#34;, TRUE ~ player )) %\u0026gt;% group_by(player) %\u0026gt;% summarize(games = n_distinct(game_id), TOI = sum(event_length) / 60) # Find basic player stats # To find individual stats, we again need to pivot the data to one row per player # but we\u0026#39;re using the event_players only (not the on ice players) # You\u0026#39;ll notice we\u0026#39;re filtering out the shootout (which is game_period 5) because # those goals don\u0026#39;t count # We\u0026#39;ll sum up blocked shots (event_player_2 is the player who blocked the shot, # event_player_1 is the one who generated it), total points, shots, unblocked shots, # hits (both give nand received) player_stats \u0026lt;- pbp %\u0026gt;% filter(event_type %in% c(\u0026#34;HIT\u0026#34;, \u0026#34;BLOCK\u0026#34;, \u0026#34;SHOT\u0026#34;, \u0026#34;MISS\u0026#34;, \u0026#34;GOAL\u0026#34;) \u0026amp; game_period \u0026lt; 5) %\u0026gt;% select(game_id, event_type, event_player_1:event_player_3) %\u0026gt;% pivot_longer(event_player_1:event_player_3, names_to = \u0026#34;number\u0026#34;, values_to = \u0026#34;player\u0026#34;) %\u0026gt;% filter(!(is.na(player))) %\u0026gt;% mutate(block = ifelse(event_type == \u0026#34;BLOCK\u0026#34; \u0026amp; number == \u0026#34;event_player_2\u0026#34;, 1, 0), point = ifelse(event_type == \u0026#34;GOAL\u0026#34;, 1, 0), shot = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;), 1, 0), fenwick = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;), 1, 0), hit = ifelse(number == \u0026#34;event_player_1\u0026#34; \u0026amp; event_type == \u0026#34;HIT\u0026#34;, 1, 0), hit_rec = ifelse(number == \u0026#34;event_player_2\u0026#34; \u0026amp; event_type == \u0026#34;HIT\u0026#34;, 1, 0), player = case_when( player == \u0026#34;COLIN.WHITE2\u0026#34; ~ \u0026#34;COLIN.WHITE\u0026#34;, player == \u0026#34;ERIK.GUSTAFSSON2\u0026#34; ~ \u0026#34;ERIK.GUSTAFSSON\u0026#34;, player == \u0026#34;PATRICK.MAROON\u0026#34; ~ \u0026#34;PAT.MAROON\u0026#34;, TRUE ~ player )) %\u0026gt;% group_by(player) %\u0026gt;% summarize(blocks = sum(block), points = sum(point), shots = sum(shot), fenwick = sum(fenwick), hits = sum(hit), hits_rec = sum(hit_rec)) # Join stats into TOI data frame and create rates player_TOI_stats \u0026lt;- player_TOI %\u0026gt;% left_join(player_stats, by = \u0026#34;player\u0026#34;) %\u0026gt;% mutate(points_60 = points * 60 / TOI, shots_60 = shots * 60 / TOI, fenwick_60 = fenwick * 60 / TOI, hits_60 = hits * 60 / TOI, hits_rec_60 = hits_rec * 60 / TOI, blocks_60 = blocks * 60 / TOI, TOI_game = TOI / games) %\u0026gt;% select(-c(blocks:hits_rec)) # Clean up the biographical data bios \u0026lt;- bios %\u0026gt;% mutate(player = str_to_upper(Player), player = str_replace(player, \u0026#34; \u0026#34;, \u0026#34;.\u0026#34;), defense = ifelse(Position == \u0026#34;D\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;F\u0026#34;)) %\u0026gt;% rename(height = `Height (in)`, weight = `Weight (lbs)`) %\u0026gt;% select(player, defense, height, weight) %\u0026gt;% mutate(player = str_replace_all(player, \u0026#34;ALEXANDER\u0026#34;, \u0026#34;ALEX\u0026#34;), player = str_replace_all(player, \u0026#34;ALEXANDRE\u0026#34;, \u0026#34;ALEX\u0026#34;), player = case_when( player == \u0026#34;CHRISTOPHER.TANEV\u0026#34; ~ \u0026#34;CHRIS.TANEV\u0026#34;, player == \u0026#34;DANNY.O\u0026#39;REGAN\u0026#34; ~ \u0026#34;DANIEL.O\u0026#39;REGAN\u0026#34;, player == \u0026#34;EVGENII.DADONOV\u0026#34; ~ \u0026#34;EVGENY.DADONOV\u0026#34;, player == \u0026#34;MATTHEW.BENNING\u0026#34; ~ \u0026#34;MATT.BENNING\u0026#34;, player == \u0026#34;MITCHELL.MARNER\u0026#34; ~ \u0026#34;MITCH.MARNER\u0026#34;, TRUE ~ player )) # Join biographical data into stats data # Filter to only keep players who played at least 20 games final_data \u0026lt;- player_TOI_stats %\u0026gt;% left_join(bios, by = \u0026#34;player\u0026#34;) %\u0026gt;% filter(games \u0026gt; 19) These stats are the ones that we're going to use in our model to predict whether a given player is a forward or a defenseman. Let's create at a few graphs, just to see how some of these data look.\n# The code for these four graphs is nearly the same, just change the x # and the title/labels final_data %\u0026gt;% ggplot(aes(x = weight, fill = defense)) + geom_density(alpha = 0.7, color = NA) + scale_fill_manual(values = c(\u0026#34;#0d324d\u0026#34;, \u0026#34;#a188a6\u0026#34;)) + labs( y = \u0026#34;Density\u0026#34;, x = \u0026#34;Weight (lbs)\u0026#34;, fill = NULL, title = \u0026#34;Weight by Position\u0026#34;, subtitle = \u0026#34;2018-19 NHL Season, 20+ Games Played Only\u0026#34;, caption = \u0026#34;Source: Natural Stat Trick\u0026#34; ) + meg_theme() + theme(legend.position = c(0.9, 0.9))             We can spot some differences here by position: defensemen tend to score at a lower rate and block shots at a higher rate than forwards do. They also tend to spend more time on the ice (by necessity, since there are generally half the number of defensemen as forwards on a dressed roster), which is one of the most well-known differences in the positions. In order to try to predict whether a given player is a forward or a defenseman, we're going to build two logistic regression models. One will have the average time on ice as its sole predictor variable, while the other will have all of these variables (average time on ice, height, weight, points per 60, shots per 60, unblocked shots per 60, hits per 60, hits received per 60, and blocked shots per 60) as predictor variables.\nGet data ready for modeling Our final_data data frame from above will be the base of our model_data (we're just removing two unnecessary variables), and we'll use set.seed() to create reproducible samples.\n# Rearrange our model data model_data \u0026lt;- final_data %\u0026gt;% select(player, defense, everything(), -c(games, TOI)) # Set the seed (very useful for reproducible samples!) set.seed(1234) # Split into training and testing data split_data \u0026lt;- initial_split(model_data, prop = 0.6, strata = defense) The last line of code above, which created a list called split_data, uses the helpful initial_split function from the rsample package. This allows us to create a training data set and a testing data set, an essential step when modeling. We will train the data on one data set and then test the models on a separate data set. You can set the proportion on your own, of how many observations will go to the training data, but it will default to 0.75 without a different specification. And why did I include defense as an optional strata argument?\n   As you can see above, our data set has nearly twice the amount of forwards as defensemen. By using the strata option, we can ensure that there's a similar proportion of forwards to defensemen in both our training and our testing data sets.\n# Create our testing and training data sets training_data \u0026lt;- training(split_data) testing_data \u0026lt;- testing(split_data) # Write the recipe for our small TOI_only model recipe_TOI_only \u0026lt;- training_data %\u0026gt;% recipe(defense ~ TOI_game) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% prep() recipe_TOI_only In the code above, we can create our basic training and test data sets and then move onto the useful functions of the recipes package. This package allows you to create a recipe in order to organize all of your processing steps for your model(s). You specify the arguments with the recipe() function and then specify processing steps with the various functions that begin with step_. There are dozens of these that will perform all sorts of functions (e.g., create dummy variables, input various values, take the log), but here we're just using step_center() and step_scale() to show you how to normalize variables. In order to specify variables for these step_ functions, you can use standard dplyr::select variables (e.g., starts_with(), ends_with()) or select by role (e.g., all_predictors(), all_outcomes()) or select by data type (e.g., all_numeric()). And you can of course select by variable name, as well.\nWe now have a recipe called recipe_TOI_only that looks like this.\n   Run our models # Extract our prepped training data  # and \u0026#34;bake\u0026#34; our testing data training_baked_TOI \u0026lt;- juice(recipe_TOI_only) testing_baked_TOI \u0026lt;- recipe_TOI_only %\u0026gt;% bake(testing_data) # Run the model with our training data logistic_glm_TOI \u0026lt;- logistic_reg(mode = \u0026#34;classification\u0026#34;) %\u0026gt;% set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% fit(defense ~ ., data = training_baked_TOI) Now that we have our recipe, we can apply it to our training and testing data. Since the training data was the base of the recipe, we can use the juice() function to extract it. And the bake() function will prep the test data. Then, we can actually run the model with functions from the parsnip package. The package handles many different kind of models, but here we're running a simple logistic regression and training it on our baked data.\n# Find the class predictions from our testing data # And add back in the true values from testing data predictions_class_TOI \u0026lt;- logistic_glm_TOI %\u0026gt;% predict(new_data = testing_baked_TOI) %\u0026gt;% bind_cols(testing_baked_TOI %\u0026gt;% select(defense)) # Find the probability predictions # And add all together predictions_TOI \u0026lt;- logistic_glm_TOI %\u0026gt;% predict(testing_baked_TOI, type = \u0026#34;prob\u0026#34;) %\u0026gt;% bind_cols(predictions_class_TOI) Now that the model has been trained, we can apply it to the testing data. The data frame we just created, predictions_TOI, looks like this. For each observation in our test data set, we have the predicted position and the probability that drove that prediction. We also brought in the defense variable from the test data set.\n   Just for fun, we can bring the player variable back from the original test data set and look at who was predicted the most incorrectly.\n# Look at who was predicted the most incorrectly # (Just for fun) most_wrong_TOI \u0026lt;- predictions_TOI %\u0026gt;% bind_cols(select(testing_data, player, TOI_game)) %\u0026gt;% mutate(incorrect = .pred_class != defense) %\u0026gt;% filter(incorrect == TRUE) %\u0026gt;% mutate(prob_actual = ifelse(defense == \u0026#34;D\u0026#34;, .pred_D, .pred_F)) %\u0026gt;% arrange(prob_actual)    As to be expected with such a simple model that's based solely on TOI, the predictions aren't so good for defensemen who don't play a lot of minutes or forwards who do. Let's move on to our kitchen sink model that includes all the variables.\n# Do the same process for our kitchen sink model recipe_kitchen_sink \u0026lt;- training_data %\u0026gt;% recipe(defense ~ weight + height + points_60 + shots_60 + fenwick_60 + hits_60 + hits_rec_60 + blocks_60 + TOI_game) %\u0026gt;% step_corr(all_predictors(), threshold = 0.8) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% prep() recipe_kitchen_sink training_baked_KS \u0026lt;- juice(recipe_kitchen_sink) testing_baked_KS \u0026lt;- recipe_kitchen_sink %\u0026gt;% bake(testing_data) # Run the model with our training data logistic_glm_KS \u0026lt;- logistic_reg(mode = \u0026#34;classification\u0026#34;) %\u0026gt;% set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% fit(defense ~ ., data = training_baked_KS) # Find the class predictions from our testing data # And add back in the true values from testing data predictions_class_KS \u0026lt;- logistic_glm_KS %\u0026gt;% predict(new_data = testing_baked_KS) %\u0026gt;% bind_cols(testing_baked_KS %\u0026gt;% select(defense)) # Find the probability predictions # And add all together predictions_KS \u0026lt;- logistic_glm_KS %\u0026gt;% predict(testing_baked_KS, type = \u0026#34;prob\u0026#34;) %\u0026gt;% bind_cols(predictions_class_KS) The code above looks very similar to the code from before, except we added an extra step in our recipe. The step_corr() function will study all the correlations among variables you specify and remove offenders, as it often isn't a good idea to have variables in your model that are highly correlated with each other. The default threshold for exclusion is 0.9, but you can specify whatever value you want. As you can see in the recipe below, our recipe automatically removed the shots_60 variable, which is (obviously) very highly correlated to the unblocked shot attempt variable, fenwick_60.\n   Evaluate our models In this section, I'm only going to show the code for one model (though we're evaluating two), but of course you would use the same code for both. (And if you were working with multiple models that you want to compare, it'd be a good idea to create functions to do these steps so that you aren't copying and pasting.)\nFirst we can create a confusion matrix, which simply plots the predicted values against the actual values.\n# Create a confusion matrix predictions_TOI %\u0026gt;% conf_mat(defense, .pred_class) %\u0026gt;% pluck(1) %\u0026gt;% as_tibble() %\u0026gt;% ggplot(aes(Prediction, Truth, alpha = n)) + geom_tile(show.legend = FALSE) + geom_text(aes(label = n), colour = \u0026#34;white\u0026#34;, alpha = 1, size = 8) + meg_theme() + theme(panel.grid.major = element_blank()) + labs( y = \u0026#34;Actual Position\u0026#34;, x = \u0026#34;Predicted Position\u0026#34;, fill = NULL, title = \u0026#34;Confusion Matrix\u0026#34;, subtitle = \u0026#34;TOI Only Model\u0026#34; )       Just from a brief look at this, the kitchen sink model clearly has higher accuracy (calculated as the number of correct predictions divided by the number of total predictions) than the TOI only model.\n# Find the accuracy predictions_TOI %\u0026gt;% accuracy(defense, .pred_class) # Find the logloss predictions_TOI %\u0026gt;% mn_log_loss(defense, .pred_D) # Find the area under the ROC curve (AUC) predictions_TOI %\u0026gt;% roc_auc(defense, .pred_D) # Create a tibble that holds all the evaluation metrics TOI_metrics \u0026lt;- tibble( \u0026#34;log_loss\u0026#34; = mn_log_loss(predictions_TOI, defense, .pred_D) %\u0026gt;% select(.estimate), \u0026#34;accuracy\u0026#34; = accuracy(predictions_TOI, defense, .pred_class) %\u0026gt;% select(.estimate), \u0026#34;auc\u0026#34; = roc_auc(predictions_TOI, defense, .pred_D) %\u0026gt;% select(.estimate) ) %\u0026gt;% unnest(everything()) %\u0026gt;% pivot_longer(everything(), names_to = \u0026#34;metric\u0026#34;, values_to = \u0026#34;value\u0026#34;) %\u0026gt;% mutate(model = \u0026#34;TOI_only\u0026#34;) The yardstick package is what holds a lot of these functions that are useful for model evaulation. We just defined accuracy, which you can calculate on your own from the confusion matrix and is also available via the accuracy() function. That's useful for determining how good the model is in a binary sense, while log loss (from the mn_log_loss() function) uses the probabilities to quantify how correct the predictions are. As an example, let's go back to our TOI only model and see that Aleksander Barkov (a forward) was given a 0.75 probability of being a defenseman. That's obviously incorrect. It's counted as an incorrect prediction for the accuracy metric, but log loss also takes into account that the prediction was quite wrong. If the prediction had instead given him a 0.51 probability of being a defenseman, the penalty would be less.\nWe can also create a tibble (a type of data frame) to hold all of these metrics. We'll use it to compare both models in a minute. The last metric included is the area under the ROC curve, known as AUC. The ROC curve graphs the false positive rate against the true positive rate and in a nutshell, quantifies how good the model is at distinguishing the groups.\nThe yardstick package also makes it really easy to graph the curve itself.\n# Look at the ROC curve predictions_TOI %\u0026gt;% roc_curve(defense, .pred_D) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path() + geom_abline(lty = 3) + coord_equal() + meg_theme() + labs( y = \u0026#34;True Positive Rate (Sensitivity)\u0026#34;, x = \u0026#34;False Positive Rate\u0026#34;, fill = NULL, title = \u0026#34;ROC Curve\u0026#34;, subtitle = \u0026#34;TOI Only Model\u0026#34; )       The ideal ROC curve is one that goes high up into the top left corner (as to maximize the area underneath it), so again, it appears that our kitchen sink model is performing better here. Lastly, let's use the tibbles we created to hold the evaulation metrics and graph to compare.\nmetrics_compare \u0026lt;- TOI_metrics %\u0026gt;% bind_rows(KS_metrics) metrics_compare %\u0026gt;% ggplot(aes(fill = model, y = value, x = metric)) + geom_bar(position = \u0026#34;dodge\u0026#34;, stat = \u0026#34;identity\u0026#34;) + scale_fill_manual(values = c(\u0026#34;#7A8B99\u0026#34;, \u0026#34;#A9DDD6\u0026#34;)) + meg_theme() + labs( y = \u0026#34;Value\u0026#34;, x = \u0026#34;Metric\u0026#34;, fill = NULL, title = \u0026#34;Comparing Our Models\u0026#34;, subtitle = \u0026#34;Higher is Better: Accuracy and AUC\\nLower is Better: Log Loss\u0026#34; ) + geom_text(aes(label = round(value, 3)), vjust = -0.5, size = 3, position = position_dodge(width= 0.9)) + theme(legend.position = c(0.86, 0.9))    We saw previously from the confusion matrices that the accuracy for the kitchen sink model is higher, and this tells us that the AUC is higher, as well, while the log loss is lower (which is good). Thanks to the evaluation metrics of the yardstick package (and there are many more than the few we viewed!), we have evidence that compared to the TOI only model, the kitchen sink model makes more accurate predictions and is better at distinguishing between the groups.\ntidymodels is a pretty neat set of packages, and I hope this little tutorial was useful in introducing some of the many features. Here are a handful of other resources I have found helpful as I continue to learn more about this package:\n https://juliasilge.com/blog/intro-tidymodels/ https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/ https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/  ","date":1583712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583712000,"objectID":"b5142e061a0e95bbcd107bd2a3d8c63a","permalink":"/post/tidymodels-intro/","publishdate":"2020-03-09T00:00:00Z","relpermalink":"/post/tidymodels-intro/","section":"post","summary":"Before we start, an important disclaimer: this is not a tutorial on how to thoughtfully build and thoroughly evaluate models. This is a gentle introduction to the tidymodels package (which, like the tidyverse, is actually a collection of packages), and in order to examine various functions and capabilities of those packages, we'll build two very simple models, using easily available NHL data, and go over a few ways to evaluate them.","tags":null,"title":"Exploring tidymodels With Hockey Data","type":"post"},{"authors":null,"categories":null,"content":"Last year, I tracked 1146 minutes of penalty kills, spread across 12 teams from the 2018-19 season. The teams were chosen semi-randomly (to get a decent distribution of shot attempt rates, both for and against), and games were selected randomly to end up with about a quarter of all penalty kill minutes for that team during 2018-19. I tracked zone time (so that I could track how well a penalty kill was able to keep a power play out of its offensive zone and also calculate shot rates for offensive zone time only), as well as zone entries and exits.\n   Selected Teams     Chicago   Colorado   Dallas   Winnipeg   Vegas   Edmonton   Vancouver   New Jersey   NY Islanders   NY Rangers   Philadelphia   Florida    The bulk of the penalty kill data is available in the slides from my OTTHAC presentation and the data visualization that accompanied it.\nPower play data However, tracking penalty kills means I automatically have some data on power plays! I ended up with some data for all 31 teams, but here I have restricted the plots to the 11 teams for which I had the most data (60 to ~100 minutes). This isn't enough game time to make sweeping conclusions, but it's enough to look at a few graphs.\n   Selected Teams     Buffalo   Calgary   Colorado   Edmonton   Florida   Nashville   Ottawa   Philadelphia   St. Louis   Vancouver   Washington    Shown below is the percent of total power play time on ice that the power play spent in its offensive zone. I was surprised to see a relatively small spread in the percentages (besides Ottawa).\n   As I mentioned previously with the penalty kill data, having zone time makes it possible to calculate a per 60 shot rate (for unblocked shot attempts, in this case) that is based on offensive zone time only, instead of just total power play time on ice. This shows how frequently teams generate shots when they're actually in the zone.\n   It's interesting to contrast Ottawa and Colorado in these two graphs. Ottawa wasn't particularly successful at staying in the offensive zone, but when they were there, they generated a lot of unblocked shot attempts. Colorado was nearly the opposite, in that they spent a lot of time in the offensive zone but weren't as prolific with the shots. The \u0026quot;regular\u0026quot; unblocked shot attempt rates for the two teams over these games were nearly identical.\n   Lastly, since I tracked zone entry data, as well, we can look at how power play teams tended to enter their own zone by examining the percentage of all entries that were dump-ins versus carries and passes. In this smallish sample there was a wide variation between teams that tended to dump the puck in more (Nashville, Calgary) and teams that ver rarely did (Colorado, Ottawa).\n","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"def377eb53bb076039b4aa5ee5ea1b0b","permalink":"/post/pk_project/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/pk_project/","section":"post","summary":"Last year, I tracked 1146 minutes of penalty kills, spread across 12 teams from the 2018-19 season. The teams were chosen semi-randomly (to get a decent distribution of shot attempt rates, both for and against), and games were selected randomly to end up with about a quarter of all penalty kill minutes for that team during 2018-19. I tracked zone time (so that I could track how well a penalty kill was able to keep a power play out of its offensive zone and also calculate shot rates for offensive zone time only), as well as zone entries and exits.","tags":null,"title":"Wrapping Up The Penalty Kill Project","type":"post"},{"authors":[],"categories":null,"content":"","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581120000,"objectID":"ba664c536a4dbe92df229013c91a56ea","permalink":"/talk/cbjhac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/cbjhac/","section":"talk","summary":"Examining the rise of the power kill.","tags":[],"title":"The Anatomy of a Power Kill","type":"talk"},{"authors":null,"categories":null,"content":"Score effects in hockey are well-known, whether you're watching the game or looking at numbers: teams that are losing tend to generate a greater share of the shot attempts. Micah Blake McCurdy developed an adjustment method for 5v5 events that is currently used to create the score- and venue-adjusted shot attempt metrics available at hockey stats websites like Natural Stat Trick and Evolving Hockey.\nSince I spend a lot of time looking at special teams data, I have long been curious as to whether score effects might also be a factor for power plays. Here, I've modified Micah's method to investigate score effects at 5v4: I created the weights, compared them to the 5v5 weights, and examined the repeatability and predictivity of the adjusted values compared to the raw ones. Investigating this problem taught me a lot of new R skills, so I've included all code and will go through the process step-by-step. (So if you're only interested in the conclusion, just scroll past the code and look at the graphs!)\nGet and prepare the data To start, I used the play-by-play query tool available at Evolving Hockey to collect all power play events from the past 10 seasons. (This was much quicker than scraping full seasons of data, and if you'd like access to the tool, support them on Patreon.) With all of the files in the same folder, instead of reading them in one-by-one, some functions from the plyr package make it easier to read them all in and combine them into one file.\nlibrary(plyr) library(readr) library(tidyverse) library(infer) # All 10 of my csv files (one for each season) are in a folder in my wd called \u0026#34;score_adj\u0026#34; # This will ID all the files and read them in as one file (score_adj_5v4_raw) mydir = \u0026#34;score_adj\u0026#34; myfiles = list.files(path=mydir, pattern=\u0026#34;*.csv\u0026#34;, full.names=TRUE) myfiles score_adj_5v4_raw = ldply(myfiles, read_csv) I filtered down to only the events that I want (all unblocked shot attempts as well as any events with time, so I can create rates) and created some new variables. Per Micah's method, all leads over 2 and below -2 were grouped.\n# We\u0026#39;ll filter down to only unblocked shot attempts and time events (to get fenwick \u0026amp; rates) # And also create variables to get the home lead and an indicator whether the event is home or away # For home lead, everything above 2 and below -2 will be grouped together score_adj_5v4 \u0026lt;- score_adj_5v4_raw %\u0026gt;% filter(event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;) | event_length \u0026gt; 0) %\u0026gt;% mutate(home_lead = home_score - away_score, home_lead = ifelse(home_lead \u0026lt;= -3, -3, ifelse(home_lead \u0026gt;= 3, 3, home_lead)), home_away = ifelse(event_team == home_team, \u0026#34;home\u0026#34;, \u0026#34;away\u0026#34;), fenwick = ifelse(event_type %in% c(\u0026#34;SHOT\u0026#34;, \u0026#34;GOAL\u0026#34;, \u0026#34;MISS\u0026#34;), 1, 0), goal = ifelse(event_type == \u0026#34;GOAL\u0026#34;, 1, 0), PP_team = ifelse(game_strength_state == \u0026#34;5v4\u0026#34;, home_team, away_team), type = ifelse(fenwick == 1 \u0026amp; PP_team == event_team, \u0026#34;PP\u0026#34;, ifelse(fenwick == 1, \u0026#34;PK\u0026#34;, NA)), PP_fenwick = ifelse(type == \u0026#34;PP\u0026#34; \u0026amp; fenwick == 1, 1, 0), f = ifelse(PP_fenwick == 1, \u0026#34;Y\u0026#34;, NA)) %\u0026gt;% select(type, PP_fenwick, f, game_strength_state, home_team, away_team, PP_team, event_team, everything())  Create the weights The first step is to determine what the weights should be. In our raw data, all unblocked shot attempts count the same: as one. But they are not all truly equal since we know that trailing teams generate more shot attempts. If the team that is currently winning by two goals generates a shot attempt, for example, that should count as more than one shot attempt because it's more difficult. And vice versa for the team that is currently losing. We can calculate how much these shot attempts should be boosted or penalized by comparing the actual values to the average values (i.e., if there was no difference).\nThe first data frame below, score_adj_f, just sums the unblocked shot attempts for each state to give us the raw values. (You'll see that the lockout-shortened season is filtered out, we'll discuss why later.) The second data frame score_adj_f_avg sums unblocked shot attempts for each score state only. Those average values get joined back into the first data frame, and then we can create a weight by dividing the true value by the average. score_adj_f_reshape just uses the pivot_wider function to reshape the values to make them more readable.\n# We filter by power play fenwick events only, then # group by home_away and home_lead score_adj_f \u0026lt;- score_adj_5v4 %\u0026gt;% filter(PP_fenwick == 1) %\u0026gt;% filter(season != 20122013) %\u0026gt;% group_by(home_lead, home_away, f) %\u0026gt;% summarize(fenwick = sum(PP_fenwick)) # Group by home_lead to get the average fenwicks score_adj_f_avg \u0026lt;- score_adj_f %\u0026gt;% group_by(home_lead) %\u0026gt;% summarize(avg = mean(fenwick)) # Join the average back into the previous data frame # and create the adjusted score by dividing the average by the raw fenwick score_adj_f \u0026lt;- score_adj_f %\u0026gt;% left_join(score_adj_f_avg, by = \u0026#34;home_lead\u0026#34;) %\u0026gt;% mutate(fenwick_adj = avg / fenwick) # Reshape to get the numbers in a more readable format score_adj_f_reshape \u0026lt;- score_adj_f %\u0026gt;% select(-c(avg, fenwick)) %\u0026gt;% pivot_wider(names_from = home_away, values_from = fenwick_adj) %\u0026gt;% select(home_lead, home, away)  This is what the score_adj_f data frame looks like before we reshape it for easier analysis. fenwick has the raw values, avg has the average values, and fenwick_adj has the weights (found by dividing avg by fenwick).\n   And this is what it looks like after.\n   Compare to 5v5 weights Now that we have our weights (shown above), we can compare to Micah's established weights for 5v5 events.\n# Read in Micah\u0026#39;s 5v5 values # And join to compare score_adj_5v5 \u0026lt;- read_csv(\u0026#34;score_adj_5v5_MBM.csv\u0026#34;) score_adj_f_reshape_compare \u0026lt;- score_adj_f_reshape %\u0026gt;% left_join(score_adj_5v5, by = \u0026#34;home_lead\u0026#34;) %\u0026gt;% mutate(diff_home = home - home_5v5, diff_away = away - away_5v5) # Create simple bar graphs to compare ggplot(data = score_adj_f_reshape_compare, aes(x = as.factor(home_lead), y = diff_home)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + labs(title = \u0026#34;Home Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Difference From 5v5 Weight\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) ggplot(data = score_adj_f_reshape_compare, aes(x = as.factor(home_lead), y = diff_away)) + geom_bar(stat = \u0026#34;identity\u0026#34;) + labs(title = \u0026#34;Away Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Difference From 5v5 Weight\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) for_comparison \u0026lt;- score_adj_f_reshape_compare %\u0026gt;% select(-c(diff_home:diff_away)) %\u0026gt;% rename(home_5v4 = home, away_5v4 = away) %\u0026gt;% pivot_longer(home_5v4:away_5v5, names_to = \u0026#34;type\u0026#34;, values_to = \u0026#34;value\u0026#34;) %\u0026gt;% mutate(home_away = substr(type, 1, 4), state = substr(type, 6, 8)) for_comparison %\u0026gt;% filter(home_away == \u0026#34;home\u0026#34;) %\u0026gt;% ggplot(aes(fill = state, x = as.character(home_lead), y = value)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Home Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Weight\u0026#34;, fill = \u0026#34;State?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_hline(yintercept = 1) + scale_fill_manual(values = c(\u0026#34;#CF8BA8\u0026#34;, \u0026#34;#DDDDDD\u0026#34;)) for_comparison %\u0026gt;% filter(home_away == \u0026#34;away\u0026#34;) %\u0026gt;% ggplot(aes(fill = state, x = as.character(home_lead), y = value)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Away Event Weights, 5v4 vs 5v5\u0026#34;, x = \u0026#34;Home Lead\u0026#34;, y = \u0026#34;Weight\u0026#34;, fill = \u0026#34;State?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_hline(yintercept = 1) + scale_fill_manual(values = c(\u0026#34;#CF8BA8\u0026#34;, \u0026#34;#DDDDDD\u0026#34;))              We can see from these comparisons that overall, the magnitude of these 5v4 weights is not too different from those at 5v5. The largest difference is when the home team is trailing by one: the 5v5 weight for the away team is 1.103. At 5v4, the weight for the away team is 1.221. (Interestingly, when the home team is leading by one, the weights are nearly identical.) This difference fits the overall pattern that you can see in the graphs: the weights for the away team at 5v4 are consistently higher than they are at 5v5, regardless of the score state, and vice versa for the home team. This could suggest that events for the home team are slightly easier to generate at 5v4, regardless of score state, and therefore the away team gets more \u0026quot;credit\u0026quot; (i.e., a higher weight) for their events.\nOur last step, before we can test the repeatability and prediction of the weights, is to join these adjusted values back into the raw event data so we can use them for comparison.\n# Join the adjusted values back into the raw data score_adj_5v4_w_values \u0026lt;- score_adj_5v4 %\u0026gt;% left_join(select(score_adj_f, home_lead, home_away, fenwick_adj, f), by = c(\u0026#34;home_lead\u0026#34;, \u0026#34;home_away\u0026#34;, \u0026#34;f\u0026#34;)) Testing repeatability and prediction To examine these adjusted values in comparison to the raw ones, we'll look at both repeatability and prediction. Repeatability is measured by how well the unblocked shot attempts in one sample of a season correlate to those in another sample, and prediction looks at how well the unblocked shot attempts in one sample correlate to goals in another. Our metric of interest in both cases will be R2, and we're curious to see whether those R2 values are higher for the adjusted values.\nThe first step is to create the data set, sampling_team, that we'll use for sampling purposes. (Again, we'll filter out the lockout-shortened season.) We'll group by game_id as well as PP_team and summarize the raw fenwick values, adjusted fenwick values, goals, and total TOI. We'll also create a unique identifier of team_season that will be important for sampling purposes.\n# Testing repeatability and prediction---- # We\u0026#39;ll test repeatability of the adjusted values, compared to the raw ones # But we\u0026#39;ll be removing the shortened season # Group by game, season, and team; sum values sampling_team \u0026lt;- score_adj_5v4_w_values %\u0026gt;% select(game_id, season, PP_team, PP_fenwick, fenwick_adj, event_length, goal) %\u0026gt;% filter(season != 20122013) %\u0026gt;% group_by(season, game_id, PP_team) %\u0026gt;% summarize(fenwick = sum(PP_fenwick, na.rm = TRUE), fen_adj = sum(fenwick_adj, na.rm = TRUE), goals = sum(goal), TOI = sum(event_length) / 60) %\u0026gt;% unite(team_season, season, PP_team, sep = \u0026#34;-\u0026#34;, remove = FALSE) That sampling_team data frame will be the starting point for the sampling function I wrote that's based on the rep_sample_n function in the infer package. In a nutshell, the function below will do the following: filter the sampling_team data frame to a specific team_season; take a sample of x number of games; split that sample into two groups; sum the unblocked shot attempts (both raw and adjusted), goals, and TOI in each group; and repeat 1000 times. The pivot_wider function at the end will just reshape the data into a format that's easier to use later.\n# The function below will filter by teamseason, sample a specified amount of games,  # split into two groups, sum fenwick and fen_adj, repeat 1000 times per season sampling_fn \u0026lt;- function(value, samplesize) { sampling_team_done \u0026lt;- sampling_team %\u0026gt;% filter(team_season == value) %\u0026gt;% rep_sample_n(size = samplesize, replace = FALSE, reps = 1000) %\u0026gt;% group_by(replicate, team_season) %\u0026gt;% mutate(game_no = row_number()) %\u0026gt;% mutate(group = game_no %% 2) %\u0026gt;% mutate(samplesize = samplesize) %\u0026gt;% group_by(replicate, group, team_season, samplesize) %\u0026gt;% summarize(fenwick = sum(fenwick, na.rm = TRUE), fen_adj = sum(fen_adj, na.rm = TRUE), TOI = sum(TOI), goals = sum(goals)) %\u0026gt;% mutate(f_60 = fenwick * 60 / TOI, f_adj_60 = fen_adj * 60 / TOI, goal_60 = goals * 60 / TOI) %\u0026gt;% pivot_wider(names_from = group, values_from = c(fenwick, fen_adj, TOI, f_60, f_adj_60, goals, goal_60)) } Once the function is written, we need to apply it! You can see that our sampling_fn function requires two arguments to be supplied: value and samplesize. The value is each individual team season, since we want the function to run over each one separately. And I was curious to see how the results vary by the sample size chosen, so the code below will run separately for sample sizes of 40, 50, 60, and 70 games. Micah's 5v5 method was tested with sample sizes of 40 games, but since 5v4 time is comparatively much more rare, I wanted to test larger sample sizes as well. (This is why we eliminated the lockout-shortened season, which only had 48 games.)\nlapply is a very useful function that will perform the sampling_fn, with the selected samplesize, over each unique value of team_season from the sampling_team data frame. And the bind_rows function will collect all of the function results into one data frame.\n# Run the function for the various sample sizes summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 40) sampling_team_40 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 50) sampling_team_50 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 60) sampling_team_60 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) summary_team \u0026lt;- lapply(unique(sampling_team$team_season), sampling_fn, samplesize = 70) sampling_team_70 \u0026lt;- bind_rows(summary_team, .id = \u0026#34;column_label\u0026#34;) I will end up with four data frames, sampling_team_40 through sampling_team_70 that are all the same size. Here is a sample:\n   replicate indicates which sample it is (each individual team_season has 1000) and the _0 and _1 appended to each metric show the two separate groups. For example, the first row here is one sample of 40 games from the Bruins' 2009-10 season. Those 40 games were divided into two groups: the first group had 111 total unblocked shot attempts, and the second group had 132. Having the data structured this way will allow us to easily calculate the correlations.\nWe'll create another function to do so, called correlations, and use the lapply function again to apply that function over our four data frames.\ncorrelations \u0026lt;- function(df) { correlation \u0026lt;- df %\u0026gt;% group_by(samplesize) %\u0026gt;% summarize(raw_f = cor(fenwick_0, fenwick_1) ^ 2, adj_f = cor(fen_adj_0, fen_adj_1) ^ 2, raw_rate = cor(f_60_0, f_60_1) ^ 2, adj_rate = cor(f_adj_60_0, f_adj_60_1) ^ 2, raw_f_pred = cor(fenwick_0, goals_1) ^ 2, adj_f_pred = cor(fen_adj_0, goals_1) ^ 2, raw_rate_pred = cor(f_60_0, goal_60_1) ^ 2, adj_rate_pred = cor(f_adj_60_0, goal_60_1) ^ 2) } cor_all \u0026lt;- lapply(list(sampling_team_40, sampling_team_50, sampling_team_60, sampling_team_70), correlations) cor \u0026lt;- bind_rows(cor_all) That will result in a very simple data frame that looks like this:\n   And we can reshape that data to more easily create some graphs.\n# Create graphs to compare---- # Reshape data to make it easier cor_reshape \u0026lt;- cor %\u0026gt;% pivot_longer(raw_f:adj_rate_pred, names_to = \u0026#34;metric\u0026#34;, values_to = \u0026#34;R2\u0026#34;) %\u0026gt;% mutate(type = substr(metric, 1, 3)) %\u0026gt;% arrange(type) %\u0026gt;% mutate(type = factor(type, levels=c(\u0026#34;raw\u0026#34;, \u0026#34;adj\u0026#34;))) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_f\u0026#34;, \u0026#34;adj_f\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Repeatability: Unblocked Shot Attempts\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_rate\u0026#34;, \u0026#34;adj_rate\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Repeatability: Unblocked Shot Attempt Rate\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) cor_reshape %\u0026gt;% filter(metric %in% c(\u0026#34;raw_rate_pred\u0026#34;, \u0026#34;adj_rate_pred\u0026#34;)) %\u0026gt;% ggplot(aes(fill = type, x = samplesize, y = R2)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;) + labs(title = \u0026#34;Predictivity: Unblocked Shot Attempt Rate\u0026#34;, x = \u0026#34;Sample Size\u0026#34;, y = \u0026#34;R-Squared\u0026#34;, fill = \u0026#34;Adjusted?\u0026#34;) + theme_linedraw() + theme(axis.ticks = element_blank()) + geom_text(aes(label = round(R2, 3)), position = position_dodge(10), vjust = -0.5, size = 3) + scale_fill_manual(values = c(\u0026#34;#DDDDDD\u0026#34;, \u0026#34;#CF8BA8\u0026#34;)) The two figures below show the R2 values for raw and adjusted values, by sample size, for the unblocked shot attempts and the unblocked shot attempt rate, respectively. As expected, the correlation increases along with the sample size. And although none of the R2 values are particularly large, they are consistently higher for the adjusted values. (As a comparison, Micah found a R2 value of 0.530 for a similar test with 5v5 adjusted values.)\n      Also as expected, the R2 value is much smaller when we look at prediction: how the unblocked shot attempt rate in one group predicts the goal rate in another group. Again as comparison: Micah used goal percentage instead for his prediction test and found a R2 value of 0.113.\n   My personal conclusion is that there appears to be some value from adjusting for score effects at 5v4, but I'm not sure it's enough to recommend score adjustment as common practice. For context, I'd also like to explore further into how score effects might affect the rate of drawing or taking penalties.\n","date":1579046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579046400,"objectID":"0ed8ca98ebb3b775ed3636b7c4c0da16","permalink":"/post/score-effects/","publishdate":"2020-01-15T00:00:00Z","relpermalink":"/post/score-effects/","section":"post","summary":"Score effects in hockey are well-known, whether you're watching the game or looking at numbers: teams that are losing tend to generate a greater share of the shot attempts. Micah Blake McCurdy developed an adjustment method for 5v5 events that is currently used to create the score- and venue-adjusted shot attempt metrics available at hockey stats websites like Natural Stat Trick and Evolving Hockey.\nSince I spend a lot of time looking at special teams data, I have long been curious as to whether score effects might also be a factor for power plays.","tags":null,"title":"Examining Score Effects on Special Teams","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nI have written a couple articles over the past few months on using R with hockey data (see here and here), but both of those articles were focused on intermediate techniques and presumed beginner knowledge of R. In contrast, this article is for the complete beginner. We’ll go through the steps of downloading and setting up R and then, with the use of a sample hockey data set, learn the very basics of R for exploring and visualizing data.\n Click here to read the rest of this article on Hockey-Graphs and here to see the code on Github.   ","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"38fec2007e46690304b837eb7d9b5a56","permalink":"/post/r-tutorial/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/post/r-tutorial/","section":"post","summary":"From Hockey-Graphs\nI have written a couple articles over the past few months on using R with hockey data (see here and here), but both of those articles were focused on intermediate techniques and presumed beginner knowledge of R. In contrast, this article is for the complete beginner. We’ll go through the steps of downloading and setting up R and then, with the use of a sample hockey data set, learn the very basics of R for exploring and visualizing data.","tags":null,"title":"An Introduction to R With Hockey Data","type":"post"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1573862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573862400,"objectID":"56257c951fac30e79ca6b96277812379","permalink":"/talk/otthac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/otthac/","section":"talk","summary":"Tracking data from the 2018-19 season to discuss broad defensive themes on the penalty kill.","tags":[],"title":"Discrete Defensive Strategies on the Penalty Kill","type":"talk"},{"authors":null,"categories":null,"content":"From Nightingale, the journal of the Data Visualization Society\nAs the 2019–20 NHL hockey season kicks off, Tableau data-vizzists (data visualizers? data viz artisans?) Meghan Hall and Sean Tierney took some time to discuss how they got started with visualizations in hockey, their processes for creating interactive charts, and what they see as the future of data visualization for the NHL.\n Click here to read the rest of this article on Medium.   ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"830a7eb642c3f30419a68341348f33ea","permalink":"/post/medium/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/medium/","section":"post","summary":"From Nightingale, the journal of the Data Visualization Society\nAs the 2019–20 NHL hockey season kicks off, Tableau data-vizzists (data visualizers? data viz artisans?) Meghan Hall and Sean Tierney took some time to discuss how they got started with visualizations in hockey, their processes for creating interactive charts, and what they see as the future of data visualization for the NHL.\n Click here to read the rest of this article on Medium.","tags":null,"title":"So You Want to Make a Hockey Data Viz?","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nWelcome to the second article in our series on basic data cleaning and data manipulation! In this article, we’re going to use play-by-play data from two NHL games and answer two questions:\n which power play unit generated the best shot rate in each game? which defenseman played the most 5v5 minutes in each game?  In the process of doing so, we’ll cover several topics of basic data manipulation in the tidyverse, including using functions, creating joins, grouping and summarizing data, and working with string data.\n Click here to read the rest of this article on Hockey-Graphs and here to see the code on Github.   ","date":1570492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570492800,"objectID":"c76a5641ebf06b396537e2c8bf47cc93","permalink":"/post/r-article-2/","publishdate":"2019-10-08T00:00:00Z","relpermalink":"/post/r-article-2/","section":"post","summary":"From Hockey-Graphs\nWelcome to the second article in our series on basic data cleaning and data manipulation! In this article, we’re going to use play-by-play data from two NHL games and answer two questions:\n which power play unit generated the best shot rate in each game? which defenseman played the most 5v5 minutes in each game?  In the process of doing so, we’ll cover several topics of basic data manipulation in the tidyverse, including using functions, creating joins, grouping and summarizing data, and working with string data.","tags":null,"title":"Exploratory Data Analysis Using Tidyverse","type":"post"},{"authors":null,"categories":null,"content":"From Hockey-Graphs\nA tutorial on how to combine NHL play-by-play data with manually-tracked data, with the help of the padr package in R.\n Click here to read this article on Hockey-Graphs.   ","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568851200,"objectID":"16057f3ab74fbecf09ccb3baa47c5833","permalink":"/post/r-article-1/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/post/r-article-1/","section":"post","summary":"From Hockey-Graphs\nA tutorial on how to combine NHL play-by-play data with manually-tracked data, with the help of the padr package in R.\n Click here to read this article on Hockey-Graphs.   ","tags":null,"title":"Combining Manually-Tracked Data with Play-by-Play Data","type":"post"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1568419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568419200,"objectID":"1f0bf0694b1ae5af9a86b5c3fac32400","permalink":"/talk/ritsac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ritsac/","section":"talk","summary":"Examining the last four seasons of NHL data to explore the trend of increasing offense on the penalty kill.","tags":[],"title":"Tracking Increasing Offense on the Penalty Kill","type":"talk"},{"authors":["Meghan Hall"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":null,"content":"   Click link above to view data   ","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"0bc95d4d7e60b7396e21935238fa87d8","permalink":"/talk/seahac/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/seahac/","section":"talk","summary":"Exploring the trend of when goalies get pulled in the NHL.","tags":[],"title":"Aggression and Success in Goalie Pulling","type":"talk"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Meghan Hall","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Meghan Hall","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]